# Critical QA Review - Cell by Cell Analysis

## Executive Summary

**Status**: ‚ö†Ô∏è **CRITICAL ISSUES FOUND** - Notebook will fail or produce incorrect results

**Critical Issues**: 5
**High Priority Issues**: 3
**Medium Priority Issues**: 2

---

## Cell-by-Cell Analysis

### ‚úÖ Cell 1-2: Configuration & Helper Functions
**Status**: ‚úÖ PASS
- Configuration variables are correct
- Helper functions match original notebook
- No issues

---

### ‚úÖ Cell 3-4: AttributeProcessor Embedding
**Status**: ‚úÖ PASS
- AttributeProcessor class correctly embedded
- UDF functions match original
- No issues

---

### ‚ö†Ô∏è Cell 5: Read CSV Files
**Status**: ‚ö†Ô∏è **MEDIUM PRIORITY**

**Issue**: No error handling for missing files or empty CSVs

**Potential Problems**:
- If CSV files don't exist, notebook will fail
- If CSV files are empty, `max_csv_date_str` will be None
- No validation of CSV schema

**Recommendation**:
```python
# Add validation
if vertices_df.count() == 0:
    raise ValueError("Vertices CSV is empty")
if csv_edges_df.count() == 0:
    raise ValueError("Edges CSV is empty")
```

---

### üî¥ Cell 6: Convert CSV to customer_identifiers Format
**Status**: ‚úÖ **CSV PARSING IS CORRECT** (Verified: CSV uses `US_12345` format)

**BUT**: üî¥ **CRITICAL ISSUE #1: Spider `src` format mismatch**

**CSV Format** (CONFIRMED by user):
- CSV `~from` = `{business_unit}_{customer_id}` (e.g., "US_12345")
- This matches the format in `customer_identifiers` table at the time of Neptune Backfill
- ‚úÖ Current notebook parsing is CORRECT for CSV

**Spider Format** (from AttributeProcessor):
- Spider `src` = `customer:{business_unit}_{customer_uuid}` (e.g., "customer:US_abc-123-def")
- Generated by `AttributeProcessor.CUSTOMER_NODE.format(...)`

**Impact**:
- üî¥ **CRITICAL**: When unioning CSV and Spider data, `src` formats are incompatible
- üî¥ **CRITICAL**: CSV: `src = "US_12345"` vs Spider: `src = "customer:US_abc-123-def"`
- üî¥ **CRITICAL**: Connected components will treat CSV and Spider customers as different nodes
- üî¥ **CRITICAL**: Graph analysis will be completely broken
- üî¥ **CRITICAL**: Parent relationships between CSV and Spider customers will fail

**Evidence**:
```python
# Line 521-525: CSV src format (CORRECT)
csv_customer_identifiers = csv_edges_df.select(
    F.col("~from").alias("src"),  # Format: "US_12345" ‚úÖ
    ...
)

# Line 765: Spider src format (MISMATCH)
F.col("customer_node").alias("src"),  # Format: "customer:US_abc-123-def" ‚ùå
```

**Fix Required**:
```python
# Normalize Spider src to match CSV format
# Remove "customer:" prefix and use customer_id instead of customer_uuid
spider_customer_identifiers = cd4_df.select(
    "business_unit",
    "customer_id",
    "customer_uuid",
    "created_at",
    "subscribed_at_local",
    # Normalize src to match CSV format
    F.concat(F.col("business_unit"), F.lit("_"), F.col("customer_id")).alias("src"),
    F.col("attribute_node.identifier").alias("dst"),
    "attribute_node.identifier_source",
    ...
)
```

**Root Cause**: 
1. CSV was created when `customer_identifiers` table used `src = {business_unit}_{customer_id}`
2. Current notebook (and AttributeProcessor) uses `src = customer:{business_unit}_{customer_uuid}`
3. These formats are incompatible

**Recommendation**: 
1. **IMMEDIATE**: Normalize Spider `src` to match CSV format: `{business_unit}_{customer_id}`
2. Keep `customer_uuid` in its own column (already present)
3. This ensures compatibility between CSV and Spider data

---

### ‚ö†Ô∏è Cell 7: Calculate Relationships for CSV Data
**Status**: ‚ö†Ô∏è **HIGH PRIORITY**

**Issue #1**: `created_at` type inconsistency

**Problem**: 
- CSV `created_at` is a string from CSV file
- After `F.to_timestamp()`, it becomes a timestamp
- But `max_csv_date_str` is still a string
- When comparing with Spider data, type mismatch may cause issues

**Evidence**:
```python
# Line 571: max_csv_date_str is string
max_csv_date_str = max_date_result['max_created_at']  # String from CSV

# Line 674: Comparing with timestamp
.filter(F.col("subscribed_at_local") > F.to_timestamp(F.lit(max_csv_date_str)))
```

**Fix**: Ensure consistent timestamp type:
```python
max_csv_date = F.to_timestamp(F.lit(max_csv_date_str))
# Then use max_csv_date in comparisons
```

**Issue #2**: Parent relationships calculated on CSV data only

**Problem**: 
- Parent relationships are calculated on CSV data in isolation
- These will be **incorrect** after combining with Spider data
- Example: If CSV customer A is parent of CSV customer B, but Spider customer C (earlier date) should be parent of B, the relationship is wrong

**Impact**:
- ‚ö†Ô∏è **MEDIUM**: Parent relationships in CSV data are temporary and will be recalculated
- ‚úÖ This is OK because we recalculate on combined data (Step 6)

**Recommendation**: Add comment explaining this is temporary

---

### ‚ö†Ô∏è Cell 8: Process Spider Data
**Status**: ‚ö†Ô∏è **HIGH PRIORITY**

**Issue #1**: Date comparison logic

**Problem**: 
```python
.filter(F.col("subscribed_at_local") > F.to_timestamp(F.lit(max_csv_date_str)))
```

**Questions**:
- Should this be `>=` or `>`?
- If CSV max date is "2025-03-05 10:30:00", should we include Spider records at exactly that timestamp?
- Current logic excludes records at the exact max date

**Recommendation**: Clarify requirement - typically `>` is correct (exclude boundary)

**Issue #2**: `count_connections` calculated on Spider data only

**Problem**:
```python
.withColumn(
    "count_connections",
    F.approxCountDistinct("customer_node").over(
        Window.partitionBy(*identifier_key_array)  # Only Spider data!
    ),
)
```

**Impact**:
- ‚ö†Ô∏è **LOW**: This is recalculated on combined data anyway (Step 6)
- But it's misleading and inefficient

**Recommendation**: Remove this calculation or add comment that it's temporary

**Issue #3**: Missing null check for `max_csv_date_str`

**Problem**: If CSV is empty, `max_csv_date_str` will be None, causing filter to fail

**Fix**:
```python
if max_csv_date_str is None:
    # Process all spider data
    spider_df = spark.table("payments_hf.checkout_customer_details_spider").select(columns)
else:
    spider_df = (
        spark.table("payments_hf.checkout_customer_details_spider")
        .filter(F.col("subscribed_at_local") > F.to_timestamp(F.lit(max_csv_date_str)))
        .select(columns)
    )
```

---

### üî¥ Cell 9: Combine CSV and Spider Data
**Status**: üî¥ **CRITICAL ISSUE #2**

**Problem**: **Union will fail due to schema mismatch**

**CSV Schema**:
- `src`: String (format: "US_12345")
- `created_at`: String or Timestamp
- `customer_uuid`: NULL

**Spider Schema**:
- `src`: String (format: "customer:US_abc-123-def")
- `created_at`: String (ISO format)
- `customer_uuid`: String

**Impact**:
- üî¥ **CRITICAL**: `unionByName` may fail if schemas don't match exactly
- üî¥ **CRITICAL**: Even if it succeeds, `src` formats are incompatible (see Issue #1)

**Evidence**:
```python
# Line 791: Union
combined_customer_identifiers = csv_customer_identifiers.unionByName(
    spider_customer_identifiers, 
    allowMissingColumns=True
)
```

**Fix Required**: 
1. Ensure `src` formats match (see Critical Issue #1)
2. Ensure `created_at` types match
3. Ensure all columns are present in both DataFrames

---

### üî¥ Cell 10: Recalculate Relationships on Combined Data
**Status**: üî¥ **CRITICAL ISSUE #3**

**Problem**: **Parent relationships may be incorrect if `src` formats don't match**

**Impact**:
- If CSV uses `src = "US_12345"` and Spider uses `src = "customer:US_abc-123"`, then:
  - Parent relationships calculated on `business_unit_customer_id` (e.g., "US_12345") will work
  - But `src` column values won't match, breaking downstream joins

**Additional Issue**: `created_at` type handling

**Problem**:
```python
F.coalesce(
    F.to_timestamp(F.col("created_at")),
    F.col("created_at")
)
```

**Issue**: If `created_at` is already a timestamp, `F.to_timestamp()` may fail or return NULL

**Fix**:
```python
F.when(
    F.col("created_at").cast("string").rlike("^\\d{4}-\\d{2}-\\d{2}"),  # Is string
    F.to_timestamp(F.col("created_at"))
).otherwise(F.col("created_at"))  # Already timestamp
```

---

### ‚úÖ Cell 11: Select Final Columns
**Status**: ‚úÖ PASS
- Column selection looks correct
- Matches expected schema

---

### ‚úÖ Cell 12: Save customer_identifiers
**Status**: ‚úÖ PASS
- Save function is correct
- Output path is correct

---

### ‚ö†Ô∏è Cell 13: Create Connected Components
**Status**: ‚ö†Ô∏è **HIGH PRIORITY**

**Issue**: **Vertices creation logic may miss customers**

**Problem**:
```python
vertices_df = (
    edges_df.withColumn(
        "id_array",
        F.array(
            F.struct(F.col("src").alias("id"), F.lit(True).alias("is_customer")),
            F.struct(F.col("dst").alias("id"), F.lit(False).alias("is_customer"))
        )
    )
    .withColumn("exploded_id", F.explode("id_array"))
    .select("exploded_id.*")
    .dropDuplicates()
)
```

**Issue**: This creates vertices from edges, but:
- If a customer has no edges (no identifiers), they won't appear in vertices
- This may be intentional (only customers with identifiers are in graph)

**Impact**: 
- ‚ö†Ô∏è **LOW**: This matches original notebook behavior
- But need to verify this is correct for downstream

---

### üî¥ Cell 14: Create Graph Customers
**Status**: üî¥ **CRITICAL ISSUE #4**

**Problem**: **Customer features extraction logic is flawed**

**Issue #1**: CSV customer features filter

```python
csv_customer_features = (
    customer_identifiers_df
    .filter(F.col("src").rlike("^[A-Z]+_[0-9]+$"))  # CSV format: business_unit_customer_id
    .select(...)
)
```

**Problem**: 
- This regex `^[A-Z]+_[0-9]+$` assumes:
  - Business unit is uppercase letters only
  - Customer ID is digits only
- But what if business unit has numbers? (e.g., "US1")
- What if customer_id has letters? (unlikely but possible)

**Issue #2**: **Spider customer features date filter**

```python
spider_customer_features = (
    spark.table("payments_hf.checkout_customer_details_spider")
    .filter(F.col("subscribed_at_local") > F.to_timestamp(F.lit(max_csv_date_str)))
    ...
)
```

**Problem**: 
- This gets customer features from `checkout_customer_details_spider`
- But we already processed Spider data and have it in `customer_identifiers_20251121`
- Why not extract from `customer_identifiers_20251121` where `src` matches Spider format?

**Issue #3**: **Missing customers**

**Problem**: 
- If a customer appears in `customer_identifiers_20251121` but:
  - CSV customer with `src = "US_12345"` format, OR
  - Spider customer with `src = "customer:US_abc-123"` format
- The filter `F.col("src").rlike("^[A-Z]+_[0-9]+$")` will only match CSV format
- Spider customers will be missing from `csv_customer_features`
- They'll be added via `spider_customer_features`, but this requires a separate query

**Impact**:
- ‚ö†Ô∏è **MEDIUM**: Inefficient (two queries instead of one)
- ‚ö†Ô∏è **MEDIUM**: Logic is confusing
- ‚úÖ Should work if date filter is correct

**Recommendation**: 
```python
# Extract ALL customer features from customer_identifiers_20251121
# Group by business_unit, customer_id to get min subscribed_at_local
customer_features_df = (
    customer_identifiers_df
    .select("business_unit", "customer_id", "subscribed_at_local")
    .groupBy("business_unit", "customer_id")
    .agg(F.min("subscribed_at_local").alias("subscribed_at_local"))
)
```

---

### ‚ö†Ô∏è Cell 15: Create Graph Customers with Match Reasons
**Status**: ‚ö†Ô∏è **MEDIUM PRIORITY**

**Issue**: SQL query references `{output_table}` which should work, but need to verify

**Potential Issue**: If `output_table` variable is not in scope for SQL, this will fail

**Fix**: Use explicit table name:
```python
spark.sql(f"""
    ...
    FROM {output_table}  # This should work
    ...
""")
```

**Status**: ‚úÖ Should work, but worth testing

---

## Summary of Critical Issues

### üî¥ Critical Issue #1: CSV/Spider `src` Format Mismatch
**Severity**: üî¥ **CRITICAL**
**Impact**: Graph analysis will be completely broken
**Fix**: Normalize `src` format between CSV and Spider data

### üî¥ Critical Issue #2: Union Schema Mismatch
**Severity**: üî¥ **CRITICAL**  
**Impact**: Union may fail or produce incorrect results
**Fix**: Ensure schemas match exactly before union

### üî¥ Critical Issue #3: Parent Relationship Calculation
**Severity**: üî¥ **CRITICAL**
**Impact**: Parent relationships will be incorrect if `src` formats don't match
**Fix**: Fix `src` format first, then verify parent relationships

### üî¥ Critical Issue #4: Customer Features Extraction
**Severity**: üî¥ **CRITICAL**
**Impact**: May miss customers or use incorrect logic
**Fix**: Simplify to extract from `customer_identifiers_20251121` directly

### ‚ö†Ô∏è High Priority Issue #1: Date Type Consistency
**Severity**: ‚ö†Ô∏è **HIGH**
**Impact**: Date comparisons may fail
**Fix**: Ensure consistent timestamp types

### ‚ö†Ô∏è High Priority Issue #2: Missing Null Checks
**Severity**: ‚ö†Ô∏è **HIGH**
**Impact**: Notebook will fail if CSV is empty
**Fix**: Add null checks and error handling

---

## Recommendations

1. **IMMEDIATE**: Fix `src` format mismatch (Critical Issue #1)
   - Determine correct format from original notebook
   - Normalize both CSV and Spider data to same format

2. **IMMEDIATE**: Fix customer features extraction (Critical Issue #4)
   - Extract directly from `customer_identifiers_20251121`
   - Remove separate CSV/Spider queries

3. **HIGH PRIORITY**: Add error handling
   - Check for empty CSVs
   - Check for null `max_csv_date_str`
   - Validate schemas before union

4. **MEDIUM PRIORITY**: Improve date handling
   - Ensure consistent timestamp types
   - Clarify boundary condition (`>` vs `>=`)

5. **LOW PRIORITY**: Code cleanup
   - Remove temporary `count_connections` calculation in Spider processing
   - Add comments explaining temporary calculations

---

## Testing Recommendations

1. **Unit Test**: Verify `src` format normalization
2. **Integration Test**: Test with empty CSV files
3. **Integration Test**: Test with CSV files that have max date = current date
4. **Integration Test**: Verify parent relationships are correct across CSV/Spider boundary
5. **Integration Test**: Verify customer features extraction includes all customers
6. **End-to-End Test**: Run full pipeline and compare results with original notebook

---

## Downstream Impact Analysis

### Impact on `connected_components_20251121`:
- ‚ö†Ô∏è **CRITICAL**: If `src` formats don't match, connected components will be incorrect
- CSV customers and Spider customers will be in separate components even if they share identifiers

### Impact on `graph_customers_20251121`:
- ‚ö†Ô∏è **HIGH**: If customer features are missing, some customers won't appear in graph_customers
- Parent relationships will be incorrect if `src` formats don't match

### Impact on `graph_customers_with_match_reasons_20251121`:
- ‚ö†Ô∏è **MEDIUM**: Match reasons depend on parent relationships, so will be incorrect if parents are wrong

---

## Conclusion

**The notebook has 4 CRITICAL issues that must be fixed before it can be used in production.**

The most critical issue is the `src` format mismatch between CSV and Spider data. This will cause the entire graph analysis to fail.

**Recommended Action**: 
1. Fix Critical Issues #1 and #4 immediately
2. Add error handling (High Priority Issues)
3. Test thoroughly before deployment

